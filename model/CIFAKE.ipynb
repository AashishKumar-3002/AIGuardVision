{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4-fcVH4GNXk_"
   },
   "outputs": [],
   "source": [
    "!pip install -U evaluate transformers datasets>=2.14.5 accelerate>=0.27 mlflow 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "K4QGt7DCNKa7"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules\n",
    "import warnings  # Import the 'warnings' module for handling warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings during execution\n",
    "\n",
    "import gc  # Import the 'gc' module for garbage collection\n",
    "import numpy as np  # Import NumPy for numerical operations\n",
    "import pandas as pd  # Import Pandas for data manipulation\n",
    "import itertools  # Import 'itertools' for iterators and looping\n",
    "from collections import Counter  # Import 'Counter' for counting elements\n",
    "import matplotlib.pyplot as plt  # Import Matplotlib for data visualization\n",
    "from sklearn.metrics import (  # Import various metrics from scikit-learn\n",
    "    accuracy_score,  # For calculating accuracy\n",
    "    roc_auc_score,  # For ROC AUC score\n",
    "    confusion_matrix,  # For confusion matrix\n",
    "    classification_report,  # For classification report\n",
    "    f1_score  # For F1 score\n",
    ")\n",
    "\n",
    "# Import custom modules and classes\n",
    "from imblearn.over_sampling import RandomOverSampler # import RandomOverSampler\n",
    "import accelerate # Import the 'accelerate' module\n",
    "import evaluate  # Import the 'evaluate' module\n",
    "from datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes\n",
    "from transformers import (  # Import various modules from the Transformers library\n",
    "    TrainingArguments,  # For training arguments\n",
    "    Trainer,  # For model training\n",
    "    ViTImageProcessor,  # For processing image data with ViT models\n",
    "    ViTForImageClassification,  # ViT model for image classification\n",
    "    DefaultDataCollator  # For collating data in the default way\n",
    ")\n",
    "import torch  # Import PyTorch for deep learning\n",
    "from torch.utils.data import DataLoader  # For creating data loaders\n",
    "from torchvision.transforms import (  # Import image transformation functions\n",
    "    CenterCrop,  # Center crop an image\n",
    "    Compose,  # Compose multiple image transformations\n",
    "    Normalize,  # Normalize image pixel values\n",
    "    RandomRotation,  # Apply random rotation to images\n",
    "    RandomResizedCrop,  # Crop and resize images randomly\n",
    "    RandomHorizontalFlip,  # Apply random horizontal flip\n",
    "    RandomAdjustSharpness,  # Adjust sharpness randomly\n",
    "    Resize,  # Resize images\n",
    "    ToTensor  # Convert images to PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uc8obTlQNuBM"
   },
   "outputs": [],
   "source": [
    "# Import the necessary module from the Python Imaging Library (PIL).\n",
    "from PIL import ImageFile\n",
    "\n",
    "# Enable the option to load truncated images.\n",
    "# This setting allows the PIL library to attempt loading images even if they are corrupted or incomplete.\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jiv-7cMxNy2n",
    "outputId": "54f6cde3-2c87-4980-e3d7-80460902d69c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000 120000\n",
      "(120000, 2)\n"
     ]
    }
   ],
   "source": [
    "# use https://huggingface.co/docs/datasets/image_load for reference\n",
    "\n",
    "# Import necessary libraries\n",
    "image_dict = {}\n",
    "\n",
    "# Define the list of file names\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# Initialize empty lists to store file names and labels\n",
    "file_names = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through all image files in the specified directory\n",
    "for file in sorted((Path('./kaggle/input/cifake-real-fake/').glob('*/*/*.*'))):\n",
    "    label = str(file).split('/')[-2]  # Extract the label from the file path\n",
    "    labels.append(label)  # Add the label to the list\n",
    "    file_names.append(str(file))  # Add the file path to the list\n",
    "\n",
    "# Print the total number of file names and labels\n",
    "print(len(file_names), len(labels))\n",
    "\n",
    "# Create a pandas dataframe from the collected file names and labels\n",
    "df = pd.DataFrame.from_dict({\"image\": file_names, \"label\": labels})\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "QARCj5Y1N1nP",
    "outputId": "06bcefbc-0591-4ef7-ec0a-18ef51bba4f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kaggle/input/cifake-real-fake/test/FAKE/0 (10)...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kaggle/input/cifake-real-fake/test/FAKE/0 (2).jpg</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kaggle/input/cifake-real-fake/test/FAKE/0 (3).jpg</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kaggle/input/cifake-real-fake/test/FAKE/0 (4).jpg</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaggle/input/cifake-real-fake/test/FAKE/0 (5).jpg</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image label\n",
       "0  kaggle/input/cifake-real-fake/test/FAKE/0 (10)...  FAKE\n",
       "1  kaggle/input/cifake-real-fake/test/FAKE/0 (2).jpg  FAKE\n",
       "2  kaggle/input/cifake-real-fake/test/FAKE/0 (3).jpg  FAKE\n",
       "3  kaggle/input/cifake-real-fake/test/FAKE/0 (4).jpg  FAKE\n",
       "4  kaggle/input/cifake-real-fake/test/FAKE/0 (5).jpg  FAKE"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhHl11cUNz3t",
    "outputId": "29ea2518-e32a-4441-dad6-0f9f4efacd7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FAKE', 'REAL'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "POTyiURiN_v8",
    "outputId": "d8ff7a3b-573d-4246-e33f-f1825d43b83d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 2)\n"
     ]
    }
   ],
   "source": [
    "# random oversampling of minority class\n",
    "# 'y' contains the target variable (label) we want to predict\n",
    "y = df[['label']]\n",
    "\n",
    "# Drop the 'label' column from the DataFrame 'df' to separate features from the target variable\n",
    "df = df.drop(['label'], axis=1)\n",
    "\n",
    "# Create a RandomOverSampler object with a specified random seed (random_state=83)\n",
    "ros = RandomOverSampler(random_state=83)\n",
    "\n",
    "# Use the RandomOverSampler to resample the dataset by oversampling the minority class\n",
    "# 'df' contains the feature data, and 'y_resampled' will contain the resampled target variable\n",
    "df, y_resampled = ros.fit_resample(df, y)\n",
    "\n",
    "# Delete the original 'y' variable to save memory as it's no longer needed\n",
    "del y\n",
    "\n",
    "# Add the resampled target variable 'y_resampled' as a new 'label' column in the DataFrame 'df'\n",
    "df['label'] = y_resampled\n",
    "\n",
    "# Delete the 'y_resampled' variable to save memory as it's no longer needed\n",
    "del y_resampled\n",
    "\n",
    "# Perform garbage collection to free up memory used by discarded variables\n",
    "gc.collect()\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ws7ylj7VOB54"
   },
   "outputs": [],
   "source": [
    "# Create a dataset from a Pandas DataFrame.\n",
    "dataset = Dataset.from_pandas(df).cast_column(\"image\", Image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sh-oJ9xKODJx"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDkdN8PmXHFdLD4Xjit2nuHWOFBlnc4AFXtPvNJtlBluoiR/DGd5P5Vh+IdbnvvHOjaeC8GmxXEEhhJwWO7O5v8O1bx5mcjdNaXL39j6VNFvind1I4ZbeQqfx24rPuPDUcsXm27rJHkjcvqK6jxJ4w8JwWVhp+rxX0wi3bWtAAVOOQdxHf+VUNM8W+HdWjTSdKOoq6K8g+1hSFUdgQfqarn00GoI8jsor+6zEizN69QB71sWVrNaa5BK5Z0tpommfqQSeOnqa2dO8SlowkkhIxjrSXUUN3K80d1IjvjPII46cEYP5VxwpyjPmvoP2aPQ7e3hv8AUgHnh08xWn+qnswyyt03Hcv8JIzj+9WBrXjm00bxFcWUfg7S0minNuXjYqxB78DvXMpPqMV6lwb+1mKKVAljMZwT6oadqOpXl1cC4eOy8wYPyXk+1iABkqeO3PrWy0LP/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first image in the dataset\n",
    "dataset[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "IsQG_U2XOE23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAKE', 'FAKE', 'FAKE', 'FAKE', 'FAKE']\n"
     ]
    }
   ],
   "source": [
    "# Extracting a subset of elements from the 'labels' list using slicing.\n",
    "# The slicing syntax [:5] selects elements from the beginning up to (but not including) the 5th element.\n",
    "# This will give us the first 5 elements of the 'labels' list.\n",
    "# The result will be a new list containing these elements.\n",
    "labels_subset = labels[:5]\n",
    "\n",
    "# Printing the subset of labels to inspect the content.\n",
    "print(labels_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oyx3qO6ROHLI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of IDs to Labels: {0: 'REAL', 1: 'FAKE'} \n",
      "\n",
      "Mapping of Labels to IDs: {'REAL': 0, 'FAKE': 1}\n"
     ]
    }
   ],
   "source": [
    "# Create a list of unique labels by converting 'labels' to a set and then back to a list\n",
    "labels_list = ['REAL', 'FAKE'] #list(set(labels))\n",
    "\n",
    "# Initialize empty dictionaries to map labels to IDs and vice versa\n",
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "# Iterate over the unique labels and assign each label an ID, and vice versa\n",
    "for i, label in enumerate(labels_list):\n",
    "    label2id[label] = i  # Map the label to its corresponding ID\n",
    "    id2label[i] = label  # Map the ID to its corresponding label\n",
    "\n",
    "# Print the resulting dictionaries for reference\n",
    "print(\"Mapping of IDs to Labels:\", id2label, '\\n')\n",
    "print(\"Mapping of Labels to IDs:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "5chREDY9OKPX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120000/120000 [00:00<00:00, 796818.02 examples/s]\n",
      "Casting the dataset: 100%|██████████| 120000/120000 [00:00<00:00, 12264345.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating classlabels to match labels to IDs\n",
    "ClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)\n",
    "\n",
    "# Mapping labels to IDs\n",
    "def map_label2id(example):\n",
    "    example['label'] = ClassLabels.str2int(example['label'])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(map_label2id, batched=True)\n",
    "\n",
    "# Casting label column to ClassLabel Object\n",
    "dataset = dataset.cast_column('label', ClassLabels)\n",
    "\n",
    "# Splitting the dataset into training and testing sets using an 90-10 split ratio.\n",
    "dataset = dataset.train_test_split(test_size=0.1, shuffle=True, stratify_by_column=\"label\")\n",
    "\n",
    "# Extracting the training data from the split dataset.\n",
    "train_data = dataset['train']\n",
    "\n",
    "# Extracting the testing data from the split dataset.\n",
    "test_data = dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "PmaoGPoGOK85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:  224\n"
     ]
    }
   ],
   "source": [
    "# Define the pre-trained ViT model string\n",
    "model_str = \"dima806/ai_vs_real_image_detection\" #'google/vit-base-patch16-224-in21k'\n",
    "\n",
    "# Create a processor for ViT model input from the pre-trained model\n",
    "processor = ViTImageProcessor.from_pretrained(model_str)\n",
    "\n",
    "# Retrieve the image mean and standard deviation used for normalization\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "\n",
    "# Get the size (height) of the ViT model's input images\n",
    "size = processor.size[\"height\"]\n",
    "print(\"Size: \", size)\n",
    "\n",
    "# Define a normalization transformation for the input images\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "\n",
    "# Define a set of transformations for training data\n",
    "_train_transforms = Compose(\n",
    "    [\n",
    "        Resize((size, size)),             # Resize images to the ViT model's input size\n",
    "        RandomRotation(90),               # Apply random rotation\n",
    "        RandomAdjustSharpness(2),         # Adjust sharpness randomly\n",
    "        ToTensor(),                       # Convert images to tensors\n",
    "        normalize                         # Normalize images using mean and std\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a set of transformations for validation data\n",
    "_val_transforms = Compose(\n",
    "    [\n",
    "        Resize((size, size)),             # Resize images to the ViT model's input size\n",
    "        ToTensor(),                       # Convert images to tensors\n",
    "        normalize                         # Normalize images using mean and std\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a function to apply training transformations to a batch of examples\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "# Define a function to apply validation transformations to a batch of examples\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Eifm79dzORe3"
   },
   "outputs": [],
   "source": [
    "# Set the transforms for the training data\n",
    "train_data.set_transform(train_transforms)\n",
    "\n",
    "# Set the transforms for the test/validation data\n",
    "test_data.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ebTyCX_6OWRx"
   },
   "outputs": [],
   "source": [
    "# Define a collate function that prepares batched data for model training.\n",
    "def collate_fn(examples):\n",
    "    # Stack the pixel values from individual examples into a single tensor.\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "\n",
    "    # Convert the label strings in examples to corresponding numeric IDs using label2id dictionary.\n",
    "    labels = torch.tensor([example['label'] for example in examples])\n",
    "\n",
    "    # Return a dictionary containing the batched pixel values and labels.\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "4tRL_ZMrOYfo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.800194\n"
     ]
    }
   ],
   "source": [
    "# Create a ViTForImageClassification model from a pretrained checkpoint with a specified number of output labels.\n",
    "model = ViTForImageClassification.from_pretrained(model_str, num_labels=len(labels_list))\n",
    "\n",
    "# Configure the mapping of class labels to their corresponding indices for later reference.\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "# Calculate and print the number of trainable parameters in millions for the model.\n",
    "print(model.num_parameters(only_trainable=True) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "4Ko0d5-8Oar8"
   },
   "outputs": [],
   "source": [
    "# Load the accuracy metric from a module named 'evaluate'\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define a function 'compute_metrics' to calculate evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    # Extract model predictions from the evaluation prediction object\n",
    "    predictions = eval_pred.predictions\n",
    "\n",
    "    # Extract true labels from the evaluation prediction object\n",
    "    label_ids = eval_pred.label_ids\n",
    "\n",
    "    # Calculate accuracy using the loaded accuracy metric\n",
    "    # Convert model predictions to class labels by selecting the class with the highest probability (argmax)\n",
    "    predicted_labels = predictions.argmax(axis=1)\n",
    "\n",
    "    # Calculate accuracy score by comparing predicted labels to true labels\n",
    "    acc_score = accuracy.compute(predictions=predicted_labels, references=label_ids)['accuracy']\n",
    "\n",
    "    # Return the computed accuracy as a dictionary with the key \"accuracy\"\n",
    "    return {\n",
    "        \"accuracy\": acc_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "AznlGOBSOeUy"
   },
   "outputs": [],
   "source": [
    "# Define the name of the evaluation metric to be used during training and evaluation.\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "# Define the name of the model, which will be used to create a directory for saving model checkpoints and outputs.\n",
    "model_name = \"ai_vs_real_image_detection\"\n",
    "\n",
    "# Define the number of training epochs for the model.\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Create an instance of TrainingArguments to configure training settings.\n",
    "args = TrainingArguments(\n",
    "    # Specify the directory where model checkpoints and outputs will be saved.\n",
    "    output_dir=model_name,\n",
    "\n",
    "    # Specify the directory where training logs will be stored.\n",
    "    logging_dir='./logs',\n",
    "\n",
    "    # Define the evaluation strategy, which is performed at the end of each epoch.\n",
    "    evaluation_strategy=\"epoch\",\n",
    "\n",
    "    # Set the learning rate for the optimizer.\n",
    "    learning_rate=3e-6,\n",
    "\n",
    "    # Define the batch size for training on each device.\n",
    "    per_device_train_batch_size=64,\n",
    "\n",
    "    # Define the batch size for evaluation on each device.\n",
    "    per_device_eval_batch_size=32,\n",
    "\n",
    "    # Specify the total number of training epochs.\n",
    "    num_train_epochs=num_train_epochs,\n",
    "\n",
    "    # Apply weight decay to prevent overfitting.\n",
    "    weight_decay=0.02,\n",
    "\n",
    "    # Set the number of warm-up steps for the learning rate scheduler.\n",
    "    warmup_steps=50,\n",
    "\n",
    "    # Disable the removal of unused columns from the dataset.\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    # Define the strategy for saving model checkpoints (per epoch in this case).\n",
    "    save_strategy='epoch',\n",
    "\n",
    "    # Load the best model at the end of training.\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    # Limit the total number of saved checkpoints to save space.\n",
    "    save_total_limit=1,\n",
    "\n",
    "    # Specify that training progress should not be reported .\n",
    "    report_to=\"none\"  # log to none\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "TzxioeghOhVk"
   },
   "outputs": [],
   "source": [
    "# Create a Trainer instance for fine-tuning a language model.\n",
    "\n",
    "# - `model`: The pre-trained language model to be fine-tuned.\n",
    "# - `args`: Configuration settings and hyperparameters for training.\n",
    "# - `train_dataset`: The dataset used for training the model.\n",
    "# - `eval_dataset`: The dataset used for evaluating the model during training.\n",
    "# - `data_collator`: A function that defines how data batches are collated and processed.\n",
    "# - `compute_metrics`: A function for computing custom evaluation metrics.\n",
    "# - `tokenizer`: The tokenizer used for processing text data.\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "jcyXSQdlOlcn"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 02:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.04310623183846474,\n",
       " 'eval_accuracy': 0.9836666666666667,\n",
       " 'eval_runtime': 162.9104,\n",
       " 'eval_samples_per_second': 73.66,\n",
       " 'eval_steps_per_second': 2.302}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the pre-training model's performance on a test dataset.\n",
    "# This function calculates various metrics such as accuracy, loss, etc.,\n",
    "# to assess how well the model is performing on unseen data.\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "0nVC47NYOuJY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='165' max='1688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 165/1688 1:19:06 < 12:19:08, 0.03 it/s, Epoch 0.10/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start training the model using the trainer object.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-CNN/lib/python3.11/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1781\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1782\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1783\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1784\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1785\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-CNN/lib/python3.11/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training the model using the trainer object.\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxlXYibUO1-h"
   },
   "outputs": [],
   "source": [
    "# Evaluate the post-training model's performance on the validation or test dataset.\n",
    "# This function computes various evaluation metrics like accuracy, loss, etc.\n",
    "# and provides insights into how well the model is performing.\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kD3V4e-NO3rD"
   },
   "outputs": [],
   "source": [
    "# Use the trained 'trainer' to make predictions on the 'test_data'.\n",
    "outputs = trainer.predict(test_data)\n",
    "\n",
    "# Print the metrics obtained from the prediction outputs.\n",
    "print(outputs.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7_rIyfMO8DV"
   },
   "outputs": [],
   "source": [
    "# Extract the true labels from the model outputs\n",
    "y_true = outputs.label_ids\n",
    "\n",
    "# Predict the labels by selecting the class with the highest probability\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "\n",
    "# Define a function to plot a confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    This function plots a confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "        cm (array-like): Confusion matrix as returned by sklearn.metrics.confusion_matrix.\n",
    "        classes (list): List of class names, e.g., ['Class 0', 'Class 1'].\n",
    "        title (str): Title for the plot.\n",
    "        cmap (matplotlib colormap): Colormap for the plot.\n",
    "    \"\"\"\n",
    "    # Create a figure with a specified size\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Display the confusion matrix as an image with a colormap\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Define tick marks and labels for the classes on the axes\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.0f'\n",
    "    # Add text annotations to the plot indicating the values in the cells\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # Label the axes\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # Ensure the plot layout is tight\n",
    "    plt.tight_layout()\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Display accuracy and F1 score\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Get the confusion matrix if there are a small number of labels\n",
    "if len(labels_list) <= 150:\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix using the defined function\n",
    "    plot_confusion_matrix(cm, labels_list, figsize=(8, 6))\n",
    "\n",
    "# Finally, display classification report\n",
    "print()\n",
    "print(\"Classification report:\")\n",
    "print()\n",
    "print(classification_report(y_true, y_pred, target_names=labels_list, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_udNlZJ5O-Gb"
   },
   "outputs": [],
   "source": [
    "# Save the trained model: This line of code is responsible for saving the model\n",
    "# that has been trained using the trainer object. It will serialize the model\n",
    "# and its associated weights, making it possible to reload and use the model\n",
    "# in the future without the need to retrain it.\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qye6I6dbPA2d"
   },
   "outputs": [],
   "source": [
    "# Import the 'pipeline' function from the 'transformers' library.\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for image classification tasks.\n",
    "# You need to specify the 'model_name' and the 'device' to use for inference.\n",
    "# - 'model_name': The name of the pre-trained model to be used for image classification.\n",
    "# - 'device': Specifies the device to use for running the model (0 for GPU, -1 for CPU).\n",
    "pipe = pipeline('image-classification', model=model_name, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMaTS2svPCpL"
   },
   "outputs": [],
   "source": [
    "# Accessing an image from the 'test_data' dataset using index 1.\n",
    "image = test_data[1][\"image\"]\n",
    "\n",
    "# Displaying the 'image' variable.\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1QFt85RPELk"
   },
   "outputs": [],
   "source": [
    "# Apply the 'pipe' function to process the 'image' variable.\n",
    "pipe(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bgldb-T8PF1X"
   },
   "outputs": [],
   "source": [
    "# This line of code accesses the \"label\" attribute of a specific element in the test_data list.\n",
    "# It's used to retrieve the actual label associated with a test data point.\n",
    "id2label[test_data[1][\"label\"]]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch-CNN",
   "language": "python",
   "name": "pytorch-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
